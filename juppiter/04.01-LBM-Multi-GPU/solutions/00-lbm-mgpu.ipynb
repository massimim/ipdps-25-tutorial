{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad957b71-2191-4a00-9226-3bf7be96f261",
   "metadata": {},
   "source": [
    "# LBM Multi-GPU Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2079d5-0c3f-4e84-9971-6983d165d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import lbm_mgpu\n",
    "import warp as wp\n",
    "\n",
    "exercise_name = \"00-lbm-mgpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af69e82-7952-4593-8047-c6ad76ed2ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warp 1.7.1 initialized:\n",
      "   CUDA Toolkit 12.8, Driver 12.8\n",
      "   Devices:\n",
      "     \"cpu\"      : \"x86_64\"\n",
      "     \"cuda:0\"   : \"NVIDIA RTX A4000\" (16 GiB, sm_86, mempool enabled)\n",
      "   Kernel cache:\n",
      "     /root/.cache/warp/1.7.1\n"
     ]
    }
   ],
   "source": [
    "wp.clear_kernel_cache()\n",
    "\n",
    "gpus = wp.get_cuda_devices()\n",
    "\n",
    "# Only for testing, if we have only one GPU, we can oversubscribe it\n",
    "if len(gpus) == 1:\n",
    "    gpus = gpus * 4\n",
    "\n",
    "params = lbm_mgpu.Parameters(num_steps=5000,\n",
    "                        gpus=gpus ,\n",
    "                        nx=1024 ,\n",
    "                        ny=1024 ,\n",
    "                        prescribed_vel=0.5,\n",
    "                        Re=10000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59124cb6-59f3-40e1-b83a-434362b277c6",
   "metadata": {},
   "source": [
    "In the multi-GPU version of LBM, each device computes on a different portion of the domain (i.e., a domain partition). To make kernels parametric with respect to each partition, we abstract the information into a Warp class called `Partition`, as shown in the cell below. The following picture provides information on the class members.\n",
    "\n",
    "**Simplifications**: To make the exercise less complicated, we have the following assumptions:\n",
    "- Partitioning is only 1D.\n",
    "- We add halos even to the rightmost and leftmost partitions.\n",
    "- All partitions are the same size; this is taken care of by the `Parameter` class, which resets the values of `nx` and `ny` if needed.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/partitions.png\" alt=\"Domain partitioning in LBM\" width=\"50%\">\n",
    "  <figcaption><strong>Figure 3: Domain Partitioning</strong></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a059760e-c506-4a3d-9984-4a4d55eae204",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.struct\n",
    "class Partition:\n",
    "    id: wp.int32\n",
    "    num_partitions: wp.int32 # number of domain partitions  \n",
    "    slices_per_partition: wp.int32 # number ofx slices a partition that a partition holds\n",
    "    origin: wp.vec(length=2, dtype=wp.int32) # position of the lower index in the parition w.r.t. the domain origin.\n",
    "    shape: wp.vec(length=2, dtype=wp.int32) # dimention of the partition\n",
    "    shape_with_halo: wp.vec(length=2, dtype=wp.int32) # dimention of the partition including the halo slices \n",
    "    shape_domain: wp.vec(length=2, dtype=wp.int32) # dimantio of the original domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0b1ed-8593-43db-9295-3de2ab388b37",
   "metadata": {},
   "source": [
    "## Setting Up a Partition\n",
    "\n",
    "We iterate over the devices to initialize a partition for each of them.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/panda.png\" alt=\"Exercise\" width=\"7%\" style=\"float: left; margin-right: 10px; margin-bottom: 10px;\">\n",
    "  <figcaption><strong>Exercise</strong>: what are the values for shape_with_halo?\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "addbf62d-7039-4a22-8ab4-fedae890c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = []\n",
    "\n",
    "for i in range(params.num_gpsu):\n",
    "    partition = lbm_mgpu.Partition()\n",
    "    \n",
    "    partition.id = i\n",
    "    partition.num_partitions = params.num_gpsu\n",
    "    partition.slices_per_partition = params.dim[0] // params.num_gpsu\n",
    "    \n",
    "    partition.origin[0] = i * partition.slices_per_partition\n",
    "    partition.origin[1] = 0\n",
    "\n",
    "    partition.shape[0] = partition.slices_per_partition\n",
    "    partition.shape[1] = params.dim[1]\n",
    "\n",
    "    partition.shape_with_halo[0] = partition.shape[0] + 2\n",
    "    partition.shape_with_halo[1] = partition.shape[1]  # Add halo in y direction\n",
    "\n",
    "    partition.shape_domain[0] = params.dim[0]\n",
    "    partition.shape_domain[1] = params.dim[1]\n",
    "\n",
    "    partitions.append(partition)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868fbd8f-c32b-4eb9-905b-df51b747e6c2",
   "metadata": {},
   "source": [
    "## LBM Population Fields\n",
    "\n",
    "The population fields are now partitioned over multiple `wp.array`s, one for each GPU. Additionally, the shape of the arrays, as well as the access functions, should take into account the halos we added to move data between GPUs.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/panda.png\" alt=\"Exercise\" width=\"7%\" style=\"float: left; margin-right: 10px; margin-bottom: 10px;\">\n",
    "  <figcaption><strong>Exercise</strong>: Complete the parameters for allocating and accessing the arrays.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e0c27b-faa1-40e7-9003-2c40e69fcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fields(partitions):\n",
    "    fields = []\n",
    "    for i, partition in enumerate(partitions):\n",
    "        nx = partition.shape_with_halo[0]\n",
    "        ny = partition.shape_with_halo[1]\n",
    "        f = wp.zeros((params.Q, nx, ny), dtype=wp.float64, device=params.gpus[i])\n",
    "        fields.append(f)\n",
    "    return fields\n",
    "\n",
    "f_0 = get_fields(partitions)\n",
    "f_1 = get_fields(partitions)\n",
    "\n",
    "@wp.func\n",
    "def read_field(field: wp.array3d(dtype=wp.float64), card: wp.int32, xi: wp.int32, yi: wp.int32):\n",
    "    return field[card, xi + 1, yi]\n",
    "\n",
    "@wp.func\n",
    "def write_field(field: wp.array3d(dtype=wp.float64), card: wp.int32, xi: wp.int32, yi: wp.int32,\n",
    "                value: wp.float64):\n",
    "    field[card, xi + 1, yi] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94627deb-240a-48fe-8171-06b359612c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the memory\n",
    "mem = lbm_mgpu.Memory(params,\n",
    "                      partitions,\n",
    "                      f_0=f_0,\n",
    "                      f_1=f_1,\n",
    "                      read=read_field,\n",
    "                      write=write_field)\n",
    "\n",
    "# Initialize the kernels\n",
    "functions = lbm_mgpu.Functions(params)\n",
    "kernels = lbm_mgpu.Kernels(params, mem)\n",
    "\n",
    "Q = params.Q\n",
    "D = params.D\n",
    "bc_bulk = params.bc_bulk\n",
    "c_dev = params.c_dev\n",
    "\n",
    "# Getting some functions from our lbm library\n",
    "compute_boundaries = functions.get_apply_boundary_conditions()\n",
    "compute_macroscopic = functions.get_macroscopic()\n",
    "compute_equilibrium = functions.get_equilibrium()\n",
    "compute_collision = functions.get_kbc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71211153-b584-4cae-9707-db70f1d94cbc",
   "metadata": {},
   "source": [
    "## LBM Kernel\n",
    "\n",
    "We add the partition information to the LBM kernel, which we will need during the `stream` operator if the neighbor is outside the problem domain. Note that the partition edges do not align with the domain edges.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/panda.png\" alt=\"Exercise\" width=\"7%\" style=\"float: left; margin-right: 10px; margin-bottom: 10px;\">\n",
    "  <figcaption><strong>Exercise</strong>: What check should we use to set the `outside_domain` variable?</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81f39f2-7199-4059-806c-94c559fa42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def fused(\n",
    "        partition: lbm_mgpu.Partition,\n",
    "        omega: wp.float64,\n",
    "        f_in: wp.array3d(dtype=wp.float64),\n",
    "        bc_type_field: wp.array2d(dtype=wp.uint8),\n",
    "        f_out: wp.array3d(dtype=wp.float64),\n",
    "):\n",
    "    # Get the global index\n",
    "    it, jt = wp.tid()\n",
    "    partition_index = wp.vec2i(it, jt)\n",
    "    domain_index = partition_index + partition.origin\n",
    "\n",
    "    f_post = wp.vec(length=Q, dtype=wp.float64)\n",
    "    bc_type = bc_type_field[partition_index[0], partition_index[1]]\n",
    "\n",
    "    for q in range(params.Q):\n",
    "        partition_pull_ngh = wp.vec2i(0, 0)\n",
    "        domain_pull_ngh = wp.vec2i(0, 0)\n",
    "\n",
    "        outside_domain = False\n",
    "\n",
    "        for d in range(D):\n",
    "            partition_pull_ngh[d] = partition_index[d] - c_dev[d, q]\n",
    "            domain_pull_ngh[d] = domain_index[d] - c_dev[d, q]\n",
    "            if domain_pull_ngh[d] < 0 or domain_pull_ngh[d] >= partition.shape_domain[d]:\n",
    "                outside_domain = True\n",
    "        if not outside_domain:\n",
    "            f_post[q] = read_field(field=f_in, card=q, xi=partition_pull_ngh[0], yi=partition_pull_ngh[1])\n",
    "\n",
    "    if bc_type != bc_bulk:\n",
    "        f_post = compute_boundaries(bc_type)\n",
    "\n",
    "    mcrpc = compute_macroscopic(f_post)\n",
    "\n",
    "    # Compute the equilibrium\n",
    "    f_eq = compute_equilibrium(mcrpc)\n",
    "\n",
    "    f_post = compute_collision(f_post, f_eq, mcrpc, omega)\n",
    "\n",
    "    # Set the output\n",
    "    for q in range(params.Q):\n",
    "        write_field(field=f_out, card=q, xi=partition_index[0], yi=partition_index[1], value=f_post[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0732ff1-c784-4d5a-8b7d-9b3b2bfe2f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module lbm_mgpu.kernels 0620e2b load on device 'cuda:0' took 148.89 ms  (compiled)\n",
      "Module lbm_mgpu.kernels 9701ec0 load on device 'cuda:0' took 309.66 ms  (compiled)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lbm_mgpu.setup_LDC_problem(params=params, partitions=partitions, mem=mem)\n",
    "lbm_mgpu.export_setup(prefix=exercise_name, params=params, partitions=partitions, mem=mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b6fd5-819b-4f3f-a9cb-7bc9bbfe07ae",
   "metadata": {},
   "source": [
    "Finally, the more interesting part: defining the iteration. First, we need to handle the halo update operation, and then perform the computation.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/panda.png\" alt=\"Exercise\" width=\"7%\" style=\"float: left; margin-right: 10px; margin-bottom: 10px;\">\n",
    "  <figcaption><strong>Exercise</strong>: Complete the parameters for the device-to-device copies and for the kernel launch.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c276c7-ec8d-4d6a-a844-4868a761bdbc",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"img/panda.png\" alt=\"Exercise\" width=\"7%\" style=\"float: left; margin-right: 10px; margin-bottom: 10px;\">\n",
    "  <figcaption><strong>Exercise</strong>: Are all those `wp.synchronize()` calls mandatory?\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba29354-f421-4516-8258-a63e3328522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate():\n",
    "    for i , p in enumerate(partitions):\n",
    "        for q in range(params.Q):\n",
    "            if i != params.num_gpsu - 1:\n",
    "                src = mem.f_0[i+1][q,1]\n",
    "                dst = mem.f_0[i][q,partition.shape_with_halo[0]-1]\n",
    "                wp.copy(src=src, dest=dst, count=p.shape[1])\n",
    "            if i != 0:\n",
    "                src = mem.f_0[i - 1][q, partition.shape_with_halo[0] - 2]\n",
    "                dst = mem.f_0[i][q, 0]\n",
    "                wp.copy(src=src, dest=dst, count=p.shape[1])\n",
    "    wp.synchronize()\n",
    "    \n",
    "    for i , p in enumerate(partitions):\n",
    "        wp.launch(fused,\n",
    "                  dim=p.shape,\n",
    "                  inputs=[p, params.omega, mem.f_0[i], mem.bc_type[i], mem.f_1[i]],\n",
    "                  device=params.gpus[i])\n",
    "    wp.synchronize()\n",
    "    # Swap the fields\n",
    "    mem.f_0, mem.f_1 = mem.f_1, mem.f_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0762b-147b-4ed1-a342-44f2a605b34a",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "It’s time to run and check our solver’s performance.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/panda.png\" alt=\"Exercise\" width=\"7%\" style=\"float: left; margin-right: 10px; margin-bottom: 10px;\">\n",
    "  <figcaption><strong>Exercise</strong>: What is wrong with the profiler timeline?</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39721b86-cf27-4b63-9462-9e3d4b62f8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module __main__ 2a9a939 load on device 'cuda:0' took 861.39 ms  (compiled)\n",
      "Module lbm_mgpu.kernels 65155cc load on device 'cuda:0' took 467.83 ms  (compiled)\n",
      "--------------------------------------------\n",
      "Exercise: 00-lbm-mgpu\n",
      "Main loop time: 19.495 seconds\n",
      "MLUPS:          268.9\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Warm up iteration\n",
    "iterate()\n",
    "\n",
    "# Wait for the warm-up to finish\n",
    "wp.synchronize()\n",
    "# Start timer\n",
    "start = time.time()\n",
    "for it in range(params.num_steps):\n",
    "    iterate()\n",
    "\n",
    "wp.synchronize()\n",
    "stop = time.time()\n",
    "\n",
    "lbm_mgpu.export_final(prefix=exercise_name, params=params, partitions=partitions, mem=mem, f=mem.f_0)\n",
    "\n",
    "# Printing some statistics.\n",
    "elapsed_time = stop - start\n",
    "mlups = params.compute_mlups(elapsed_time)\n",
    "print(f\"--------------------------------------------\")\n",
    "print(f\"Exercise: {exercise_name}\")\n",
    "print(f\"Main loop time: {elapsed_time:5.3f} seconds\")\n",
    "print(f\"MLUPS:          {mlups:5.1f}\")\n",
    "print(f\"--------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
